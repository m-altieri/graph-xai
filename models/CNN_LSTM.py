# -*- coding: utf-8 -*-
"""CNN_LSTM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19HVZMYpmwa1alRNOG6yHIj6NjhiTArbc
"""

import os
import logging
import numpy as np
import tensorflow as tf

import pytftk.autocorrelation


class CNN_LSTM(tf.keras.Model):
    """
    Implementazione di CNN-LSTM.
    I dati di input sono in formato [Batch, History steps, Nodes, Features].
    Prima vengono concatenate tutte le feature per ogni nodo, in modo da avere [Batch, History steps, Nodes*Features],
    poi viene applicato un layer Conv1D lungo le feature, che produce N filtri da H step.
    La matrice di output [Batch, History steps, Nodes] viene data alla LSTM.
    Alla fine della LSTM, essa continua per ulteriori P step, gli hidden state dei quali saranno le predizioni.

    Se invece viene passato il kwarg has_dense=True, gli hidden state passano prima da un layer dense, che resituisce le predizioni.
    Con has_dense=True dovrebbe andare meglio di circa il 10%.
    """

    def __init__(self, nodes, features, prediction_steps, **kwargs):
        super(CNN_LSTM, self).__init__()

        self.logger = logging.getLogger(__name__)
        self.logger.info(__name__ + " initializing.")

        self.nodes = nodes
        self.features = features
        self.P = prediction_steps

        self.autocorrelation = kwargs.get("autocorrelation", False)

        self.CNN = tf.keras.layers.Conv1D(
            filters=(
                self.nodes * self.features if self.autocorrelation else self.features
            ),
            kernel_size=1,
        )  # REVERT era filters = self.features
        self.cell = tf.keras.layers.LSTMCell(
            self.nodes * self.features if self.autocorrelation else self.features
        )  # REVERT self.features
        self.dense = tf.keras.layers.Dense(self.nodes)
        self.adj = kwargs.get("adj")

        self.logger.info(__name__ + " initialized.")

        if self.autocorrelation:
            self.logbook = pytftk.autocorrelation.Logbook()

    def call(self, inputs):
        B, H, N, F = inputs.shape

        if self.autocorrelation:
            self.logbook.new()
            self.logbook.register(
                "I", (I := pytftk.autocorrelation.morans_I(inputs[:, 0], self.adj))
            )
            print(I)

        inputs = tf.reshape(inputs, (B, H, N * F))  # [B,H,NF]

        inputs = self.CNN(inputs)  # [B, H, F], perch√® F filtri

        preds = []
        carry = [
            tf.zeros((B, N * F if self.autocorrelation else F)),  # REVERT era F
            tf.zeros((B, N * F if self.autocorrelation else F)),  # REVERT era F
        ]  # Initial states, matrici di 0 da [B, F]

        for h in range(H):
            memory, carry = self.cell(
                inputs[:, h, :], carry
            )  # Memory: [B, F], Carry: [2, [B, F]]; memory e carry[0] sono identici

            if self.autocorrelation:
                self.logbook.register(
                    "I",
                    (
                        I := pytftk.autocorrelation.morans_I(
                            tf.reshape(memory, (B, N, F)), self.adj
                        )
                    ),
                )
                print(I)
        for p in range(self.P):
            memory, carry = self.cell(
                memory, carry
            )  # Memory: [B, F], Carry: [2, [B, F]]; memory e carry[0] sono identici
            preds.append(memory)  # [p, [B, F]]

        res = tf.transpose(preds, perm=[1, 0, 2])  # [B, P, F]
        res = self.dense(res)  # [B, P, N]

        if self.autocorrelation:
            path = f"../spatial_ac/CNN-LSTM-{self.nodes}"
            if not os.path.exists(path):
                os.makedirs(path)
            self.logbook.save_plot(path)

        return res
